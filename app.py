# -*- coding: utf-8 -*-
"""
è«–æ–‡æ¤œç´¢ï¼ˆçµ±ä¸€UIç‰ˆï¼šãŠæ°—ã«å…¥ã‚Šã«ã‚¿ã‚°ã‚’â€œè¡¨ã§ç›´æ¥å…¥åŠ›â€ï¼‰

æ©Ÿèƒ½:
- ç™ºè¡Œå¹´ãƒ¬ãƒ³ã‚¸ã€å·»ãƒ»å·ï¼ˆè¤‡æ•°é¸æŠï¼‰ã€è‘—è€…ï¼ˆæ­£è¦åŒ–ãƒ»è¤‡æ•°é¸æŠï¼‰ã€å¯¾è±¡ç‰©/ç ”ç©¶ã‚¿ã‚¤ãƒ—ï¼ˆéƒ¨åˆ†ä¸€è‡´ãƒ»è¤‡æ•°é¸æŠï¼‰
- ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ AND/OR æ¤œç´¢ï¼ˆç©ºç™½/ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã€pdf_text ã‚’å«ã‚ã‚‹ã‹é¸æŠå¯èƒ½ï¼‰
- æ¤œç´¢çµæœãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆä¸è¦åˆ—ã®éè¡¨ç¤ºã€HP/PDF ã®ãƒªãƒ³ã‚¯åŒ–ã€â˜…ã§ãŠæ°—ã«å…¥ã‚Šï¼‰
- ãŠæ°—ã«å…¥ã‚Šä¸€è¦§ï¼ˆå¸¸è¨­ãƒ»â˜…ã§è§£é™¤/è¿½åŠ ï¼‰
- ãŠæ°—ã«å…¥ã‚Šã‚¿ã‚°ä»˜ã‘ï¼šãŠæ°—ã«å…¥ã‚Šè¡¨ã®ã€Œtagsã€åˆ—ã‚’ç›´æ¥ç·¨é›†ï¼ˆã‚«ãƒ³ãƒ/ç©ºç™½åŒºåˆ‡ã‚Šï¼‰
- ã€ŒâŒ å…¨ã¦å¤–ã™ã€ãƒœã‚¿ãƒ³ã§ãŠæ°—ã«å…¥ã‚Šä¸€æ‹¬è§£é™¤
"""

import io, re, time
import pandas as pd
import requests
import streamlit as st
from pathlib import Path

# -------------------- ãƒšãƒ¼ã‚¸è¨­å®š --------------------
st.set_page_config(page_title="è«–æ–‡æ¤œç´¢ï¼ˆçµ±ä¸€UIç‰ˆï¼‰", layout="wide")


# -------------------- ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆï¼ˆè‘—è€…ãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³å¼·åŒ–ç‰ˆï¼‰ --------------------
st.markdown(
    """
    <style>
    /* ========= ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›æ¬„ï¼ˆæ ç·šã‚ã‚Šï¼‰ ========= */
    .stTextInput input, .stNumberInput input, textarea {
      background-color: #e0e0e0 !important;
      color: #000 !important;
      border: 1px solid #666 !important;
      border-radius: 6px !important;
      padding: 4px 8px !important;
    }

    /* ========= Select / MultiSelectï¼ˆå¤–æ ãƒ‡ã‚¶ã‚¤ãƒ³ï¼‰ ========= */
    .stMultiSelect div[data-baseweb="select"],
    .stSelectbox  div[data-baseweb="select"] {
      background-color: #e0e0e0 !important;
      border: 1px solid #666 !important;
      border-radius: 6px !important;
    }
    div[data-baseweb="select"] > div { background: transparent !important; }
    div[data-baseweb="select"] span { color: #000 !important; }
    div[data-baseweb="select"] svg  { color: #000 !important; fill: #000 !important; }

    /* MultiSelect ã®ã‚¿ã‚° */
    div[data-baseweb="tag"] {
      background: #d5d5d5 !important;
      color: #000 !important;
      border-radius: 12px !important;
    }

    /* --- ãƒ•ã‚©ãƒ¼ã‚«ã‚¹æ™‚ã®ã‚¹ã‚¿ã‚¤ãƒ« --- */
    input:focus, textarea:focus,
    .stTextInput input:focus, .stNumberInput input:focus {
      border: 2px solid #1a73e8 !important;
      box-shadow: 0 0 4px #1a73e8 !important;
      outline: none !important;
    }
    .stMultiSelect div[data-baseweb="select"]:focus-within,
    .stSelectbox  div[data-baseweb="select"]:focus-within {
      border: 2px solid #1a73e8 !important;
      box-shadow: 0 0 4px #1a73e8 !important;
    }
    .stMultiSelect input:focus,
    .stSelectbox  input:focus {
      border: none !important;
      box-shadow: none !important;
      outline: none !important;
    }

    /* ======== ãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³ï¼ˆå€™è£œãƒªã‚¹ãƒˆï¼‰ã‚’â€œé«˜ã & å¤ªã„ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«â€ã« ======== */
    ul[role="listbox"] {
      background: #f5f5f5 !important;
      border: 1px solid #666 !important;
      max-height: 70vh !important;     /* ç”»é¢ã®7å‰²ã¾ã§é«˜ã */
      min-height: 360px !important;     /* ä½è§£åƒåº¦ã§ã‚‚ååˆ†ãªé«˜ã•ã‚’ç¢ºä¿ */
      overflow-y: auto !important;
      padding-right: 6px !important;    /* ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ãƒãƒ¼ã¶ã‚“ã®ä½™ç™½ */
      scrollbar-width: auto;            /* Firefox: å¤ªã‚ */
      scrollbar-color: #555 #e9e9e9;    /* Firefox: ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆå¼·ã‚ */
    }
    /* å€™è£œ1è¡Œã‚’å°‘ã—é«˜ãï¼å½“ãŸã‚Šåˆ¤å®šã‚’å¤§ããã—ã¦é¸ã³ã‚„ã™ã */
    li[role="option"] {
      padding: 8px 12px !important;
      line-height: 1.4 !important;
      font-size: 0.95rem !important;
    }
    li[role="option"]:hover,
    li[role="option"][aria-selected="true"] {
      background: #e0e0e0 !important;
      color: #000 !important;
    }

    /* WebKit ç³»ï¼ˆChrome/Edge/Safariï¼‰ã®ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ãƒãƒ¼ã‚’å¤ªãï¼†é«˜ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆã« */
    ul[role="listbox"]::-webkit-scrollbar {
      width: 16px;                      /* â† å¤ªãã™ã‚‹ */
    }
    ul[role="listbox"]::-webkit-scrollbar-track {
      background: #e9e9e9;
      border-radius: 8px;
    }
    ul[role="listbox"]::-webkit-scrollbar-thumb {
      background: #555;
      border-radius: 8px;
      border: 3px solid #e9e9e9;        /* å¤–å´ã«ä½™ç™½ã‚’ã¨ã£ã¦â€œå¤ªãè¦‹ã›ã‚‹â€ */
    }
    ul[role="listbox"]::-webkit-scrollbar-thumb:hover {
      background: #333;
    }
    </style>
    """,
    unsafe_allow_html=True

)

# -------------------- å®šæ•° --------------------
KEY_COLS = [
    "llm_keywords","primary_keywords","secondary_keywords","featured_keywords",
    "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰1","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰2","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰3","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰4","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰5",
    "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰6","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰7","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰8","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰9","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰10",
]
BASE_COLS = [
    "No.","ç›¸å¯¾PASS","ç™ºè¡Œå¹´","å·»æ•°","å·æ•°","é–‹å§‹ãƒšãƒ¼ã‚¸","çµ‚äº†ãƒšãƒ¼ã‚¸",
    "è«–æ–‡ã‚¿ã‚¤ãƒˆãƒ«","è‘—è€…","file_name","HPãƒªãƒ³ã‚¯å…ˆ","PDFãƒªãƒ³ã‚¯å…ˆ",
    "å¯¾è±¡ç‰©_top3","ç ”ç©¶ã‚¿ã‚¤ãƒ—_top3",
    "llm_keywords","primary_keywords","secondary_keywords","featured_keywords",
]
TARGET_ORDER = [
    "æ¸…é…’","ãƒ“ãƒ¼ãƒ«","ãƒ¯ã‚¤ãƒ³","ç„¼é…","ã‚¢ãƒ«ã‚³ãƒ¼ãƒ«é£²æ–™","ç™ºé…µä¹³ãƒ»ä¹³è£½å“",
    "é†¤æ²¹","å‘³å™Œ","ç™ºé…µé£Ÿå“","è¾²ç”£ç‰©ãƒ»æœå®Ÿ","å‰¯ç”£ç‰©ãƒ»ãƒã‚¤ã‚ªãƒã‚¹","é…µæ¯ãƒ»å¾®ç”Ÿç‰©","ã‚¢ãƒŸãƒé…¸ãƒ»ã‚¿ãƒ³ãƒ‘ã‚¯è³ª","ãã®ä»–"
]
TYPE_ORDER = [
    "å¾®ç”Ÿç‰©ãƒ»éºä¼å­é–¢é€£","é†¸é€ å·¥ç¨‹ãƒ»è£½é€ æŠ€è¡“","å¿œç”¨åˆ©ç”¨ãƒ»é£Ÿå“é–‹ç™º","æˆåˆ†åˆ†æãƒ»ç‰©æ€§è©•ä¾¡",
    "å“è³ªè©•ä¾¡ãƒ»å®˜èƒ½è©•ä¾¡","æ­´å²ãƒ»æ–‡åŒ–ãƒ»çµŒæ¸ˆ","å¥åº·æ©Ÿèƒ½ãƒ»æ „é¤ŠåŠ¹æœ","çµ±è¨ˆè§£æãƒ»ãƒ¢ãƒ‡ãƒ«åŒ–",
    "ç’°å¢ƒãƒ»ã‚µã‚¹ãƒ†ãƒŠãƒ“ãƒªãƒ†ã‚£","ä¿å­˜ãƒ»å®‰å®šæ€§","ãã®ä»–ï¼ˆç ”ç©¶ã‚¿ã‚¤ãƒ—ï¼‰"
]

# -------------------- ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ --------------------
def norm_space(s: str) -> str:
    s = str(s or "")
    s = s.replace("\u00A0", " ")
    return re.sub(r"\s+", " ", s).strip()

def norm_key(s: str) -> str:
    return norm_space(s).lower()

AUTHOR_SPLIT_RE = re.compile(r"[;ï¼›,ã€ï¼Œ/ï¼|ï½œ]+")
def split_authors(cell):
    if not cell: return []
    return [w.strip() for w in AUTHOR_SPLIT_RE.split(str(cell)) if w.strip()]

def split_multi(s):
    if not s: return []
    return [w.strip() for w in re.split(r"[;ï¼›,ã€ï¼Œ/ï¼|ï½œ\sã€€]+", str(s)) if w.strip()]

def tokens_from_query(q):
    q = norm_key(q)
    return [t for t in re.split(r"[ ,ï¼Œã€ï¼›;ã€€]+", q) if t]

def fetch_csv(url: str) -> pd.DataFrame:
    r = requests.get(url, timeout=30)
    r.raise_for_status()
    return pd.read_csv(io.BytesIO(r.content), encoding="utf-8")

def ensure_cols(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df.columns = [str(c).strip() for c in df.columns]
    return df

def consolidate_authors_column(df: pd.DataFrame) -> pd.DataFrame:
    """è‘—è€…åˆ—ï¼šç©ºç™½ã§ã¯åˆ†å‰²ã›ãšã€åŒºåˆ‡ã‚Šè¨˜å·ã®ã¿ã§åˆ†å‰²â†’åŒåç•°è¡¨è¨˜ã‚’ä»£è¡¨è¡¨è¨˜ã«çµ±åˆ"""
    if "è‘—è€…" not in df.columns:
        return df
    df = df.copy()
    def unify(cell: str) -> str:
        names = split_authors(cell)
        seen = set()
        result = []
        for n in names:
            k = norm_key(n)
            if not k or k in seen:
                continue
            seen.add(k)
            result.append(n)
        return ", ".join(result)
    df["è‘—è€…"] = df["è‘—è€…"].astype(str).apply(unify)
    return df

def build_author_candidates(df: pd.DataFrame):
    rep = {}
    for v in df.get("è‘—è€…", pd.Series(dtype=str)).fillna(""):
        for name in split_authors(v):
            k = norm_key(name)
            if k and k not in rep:
                rep[k] = name
    return [rep[k] for k in sorted(rep.keys())]

def haystack(row):
    parts = [
        str(row.get("è«–æ–‡ã‚¿ã‚¤ãƒˆãƒ«","")),
        str(row.get("è‘—è€…","")),
        str(row.get("file_name","")),
        " ".join(str(row.get(c,"")) for c in KEY_COLS if c in row),
    ]
    return norm_key(" \n ".join(parts))

def to_int_or_none(x):
    try: return int(str(x).strip())
    except Exception:
        m = re.search(r"\d+", str(x))
        return int(m.group()) if m else None

def order_by_template(values, template):
    """1) ãƒ†ãƒ³ãƒ—ãƒ¬ã®é † 2) æœªåè¼‰ã¯ã‚¢ãƒ«ãƒ•ã‚¡é † 3) ãã®ä»–ã¯æœ€å¾Œ"""
    vs = list(dict.fromkeys(values))
    tmpl_set = set(template)
    head = [v for v in template if v in vs and "ãã®ä»–" not in v]
    mid  = sorted([v for v in vs if v not in tmpl_set and "ãã®ä»–" not in v])
    tail = [v for v in template if v in vs and "ãã®ä»–" in v] + \
           [v for v in vs if ("ãã®ä»–" in v and v not in template)]
    return head + mid + tail

def make_visible_cols(df: pd.DataFrame) -> list[str]:
    """df ã®åˆ—ã‹ã‚‰ã€ç›¸å¯¾PASS/çµ‚äº†ãƒšãƒ¼ã‚¸/file_path/num_pages/file_nameã€ã¨
       ã€llm_keywords ä»¥é™ã®å…¨åˆ—ã€ã‚’éè¡¨ç¤ºå¯¾è±¡ã«ã—ã¦ã€è¡¨ç¤ºåˆ—ãƒªã‚¹ãƒˆã‚’è¿”ã™ã€‚
    """
    base_hide = {"ç›¸å¯¾PASS", "çµ‚äº†ãƒšãƒ¼ã‚¸", "file_path", "num_pages", "file_name"}
    cols = [str(c) for c in df.columns]
    hide = set(c for c in cols if c in base_hide)
    if "llm_keywords" in cols:
        idx = cols.index("llm_keywords")
        hide.update(cols[idx:])
    return [c for c in cols if c not in hide]

def make_row_id(row):
    no = str(row.get("No.", "")).strip()
    if no and no.lower() not in {"none", "nan"}:
        return f"NO:{no}"
    ttl = str(row.get("è«–æ–‡ã‚¿ã‚¤ãƒˆãƒ«", "")).strip()
    yr  = str(row.get("ç™ºè¡Œå¹´", "")).strip()
    return f"T:{ttl}|Y:{yr}"

# -------------------- ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ --------------------
st.title("é†¸é€ å”ä¼šèªŒã€€è«–æ–‡æ¤œç´¢")

DEMO_CSV_PATH = Path("data/keywords_summary5.csv")   # ãƒ¡ã‚¤ãƒ³CSV
SUMMARY_CSV_PATH = Path("data/summaries.csv")         # â† è¿½åŠ : summary
AUTHORS_CSV_PATH = Path("data/authors_readings.csv")  # â† è¿½åŠ : è‘—è€…èª­ã¿

SECRET_URL = st.secrets.get("GSHEET_CSV_URL", "")  # ï¼ˆä»»æ„ï¼‰Secretsã«å…¥ã‚Œã¦ãŠã‘ã°è‡ªå‹•ä½¿ç”¨

@st.cache_data(ttl=600, show_spinner=False)
def load_local_csv(path: Path) -> pd.DataFrame:
    return ensure_cols(pd.read_csv(path, encoding="utf-8"))

@st.cache_data(ttl=600, show_spinner=False)
def load_url_csv(url: str) -> pd.DataFrame:
    return ensure_cols(fetch_csv(url))

# --- è¿½åŠ ï¼šsummaries.csv ãƒ­ãƒ¼ãƒ€ ---
@st.cache_data(ttl=600, show_spinner=False)
def load_summaries(path: Path) -> pd.DataFrame | None:
    try:
        if not path.exists():
            return None
        df_s = pd.read_csv(path, encoding="utf-8")
        df_s.columns = [str(c).strip() for c in df_s.columns]
        if not {"file_name", "summary"}.issubset(df_s.columns):
            return None
        return df_s[["file_name", "summary"]]
    except Exception:
        return None

# --- è¿½åŠ ï¼šauthors_readings.csv ãƒ­ãƒ¼ãƒ€ ---
@st.cache_data(ttl=600, show_spinner=False)
def load_authors_readings(path: Path) -> pd.DataFrame | None:
    try:
        if not path.exists():
            return None
        adf = pd.read_csv(path, encoding="utf-8")
        adf.columns = [str(c).strip() for c in adf.columns]
        if not {"author", "reading"}.issubset(adf.columns):
            return None
        adf["author"]  = adf["author"].astype(str).str.strip()
        adf["reading"] = adf["reading"].astype(str).str.strip()
        adf = adf[(adf["author"]!="") & (adf["reading"]!="")]
        # åŒã˜ reading ãŒè¤‡æ•°ã‚ã‚‹å ´åˆã¯å…ˆå‹ã¡
        adf = adf.drop_duplicates(subset=["reading"], keep="first")
        return adf
    except Exception:
        return None

with st.sidebar:
    st.header("ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿")
    st.caption("â€» ã¾ãšã¯ãƒ‡ãƒ¢ç”¨CSVã‚’è‡ªå‹•ãƒ­ãƒ¼ãƒ‰ã€‚URL/ãƒ•ã‚¡ã‚¤ãƒ«æŒ‡å®šã§ä¸Šæ›¸ãã§ãã¾ã™ã€‚")

    # ãƒ‡ãƒ¢è‡ªå‹•ãƒ­ãƒ¼ãƒ‰ã®ON/OFFï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆONï¼‰
    use_demo = st.toggle("ãƒ‡ãƒ¢CSVã‚’è‡ªå‹•ãƒ­ãƒ¼ãƒ‰ã™ã‚‹", value=True, help="data/demo.csv ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚")

    # ä¸Šæ›¸ãæ‰‹æ®µï¼šURL or ãƒ•ã‚¡ã‚¤ãƒ«
    url = st.text_input("å…¬é–‹CSVã®URLï¼ˆGoogleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ output=csvï¼‰", value=SECRET_URL)
    up  = st.file_uploader("CSVã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿", type=["csv"])

    # æ˜ç¤ºãƒœã‚¿ãƒ³ï¼šèª­ã¿è¾¼ã¿ï¼ˆURL/ãƒ•ã‚¡ã‚¤ãƒ«ã®å„ªå…ˆåº¦ã¯ã€Œã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ > URLã€ï¼‰
    load_clicked = st.button("èª­ã¿è¾¼ã¿ï¼ˆURL/ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å„ªå…ˆï¼‰", type="primary", key="load_btn")

# å„ªå…ˆé †ä½: 1) ã‚¯ãƒªãƒƒã‚¯ã§URL/ãƒ•ã‚¡ã‚¤ãƒ« 2) ãƒ‡ãƒ¢è‡ªå‹• 3) æœ€å¾Œã®æ‰‹æ®µï¼šå¾…æ©Ÿ
df = None
err = None

try:
    if load_clicked:
        if up is not None:
            df = ensure_cols(pd.read_csv(up, encoding="utf-8"))
            st.toast("ãƒ­ãƒ¼ã‚«ãƒ«CSVã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
        elif url.strip():
            df = load_url_csv(url.strip())
            st.toast("URLã®CSVã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
        else:
            st.warning("URL ã¾ãŸã¯ CSV ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
    elif use_demo and DEMO_CSV_PATH.exists():
        df = load_local_csv(DEMO_CSV_PATH)
        st.caption(f"âœ… ãƒ‡ãƒ¢CSVã‚’è‡ªå‹•ãƒ­ãƒ¼ãƒ‰ä¸­: {DEMO_CSV_PATH}")
    elif SECRET_URL:
        df = load_url_csv(SECRET_URL)
        st.caption("âœ… Secretsã®URLã‹ã‚‰è‡ªå‹•ãƒ­ãƒ¼ãƒ‰ä¸­")
except Exception as e:
    err = e

if df is None:
    if err:
        st.error(f"èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {err}")
    st.info("å·¦ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ CSV ã‚’æŒ‡å®šã™ã‚‹ã‹ã€ãƒ‡ãƒ¢CSVã‚’æœ‰åŠ¹ã«ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# --- è¿½åŠ ï¼šsummary ã‚’ãƒãƒ¼ã‚¸ ---
sum_df = load_summaries(SUMMARY_CSV_PATH)
if sum_df is not None:
    df = df.merge(sum_df, on="file_name", how="left")

# -------------------- å¹´ãƒ»å·»ãƒ»å·ãƒ•ã‚£ãƒ«ã‚¿ --------------------
st.subheader("æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿")
year_vals = pd.to_numeric(df.get("ç™ºè¡Œå¹´", pd.Series(dtype=str)), errors="coerce")
if year_vals.notna().any():
    ymin_all, ymax_all = int(year_vals.min()), int(year_vals.max())
else:
    ymin_all, ymax_all = 1980, 2025

c_y, c_v, c_i = st.columns([1, 1, 1])
with c_y:
    y_from, y_to = st.slider(
        "ç™ºè¡Œå¹´ï¼ˆç¯„å›²ï¼‰", min_value=ymin_all, max_value=ymax_all,
        value=(ymin_all, ymax_all)
    )
with c_v:
    vol_candidates = sorted({v for v in (df.get("å·»æ•°", pd.Series(dtype=str)).map(to_int_or_none)).dropna().unique()})
    vols_sel = st.multiselect("å·»ï¼ˆè¤‡æ•°é¸æŠï¼‰", vol_candidates, default=[])
with c_i:
    iss_candidates = sorted({v for v in (df.get("å·æ•°", pd.Series(dtype=str)).map(to_int_or_none)).dropna().unique()})
    issues_sel = st.multiselect("å·ï¼ˆè¤‡æ•°é¸æŠï¼‰", iss_candidates, default=[])

# -------------------- æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆ1æ®µç›®ï¼šå¯¾è±¡ç‰© / ç ”ç©¶ã‚¿ã‚¤ãƒ—ï¼‰ --------------------
#st.subheader("æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿")

row1_tg, row1_tp = st.columns([1.2, 1.2])

with row1_tg:
    raw_targets = {t for v in df.get("å¯¾è±¡ç‰©_top3", pd.Series(dtype=str)).fillna("") for t in split_multi(v)}
    targets_all = order_by_template(list(raw_targets), TARGET_ORDER)
    targets_sel = st.multiselect("å¯¾è±¡ç‰©ï¼ˆè¤‡æ•°é¸æŠï¼éƒ¨åˆ†ä¸€è‡´ï¼‰", targets_all, default=[])

with row1_tp:
    raw_types = {t for v in df.get("ç ”ç©¶ã‚¿ã‚¤ãƒ—_top3", pd.Series(dtype=str)).fillna("") for t in split_multi(v)}
    types_all = order_by_template(list(raw_types), TYPE_ORDER)
    types_sel = st.multiselect("ç ”ç©¶ã‚¿ã‚¤ãƒ—ï¼ˆè¤‡æ•°é¸æŠï¼éƒ¨åˆ†ä¸€è‡´ï¼‰", types_all, default=[])

# -------------------- æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆ2æ®µç›®ï¼šè‘—è€… + ã‚¤ãƒ‹ã‚·ãƒ£ãƒ«ãƒ©ã‚¸ã‚ªæ¨ªä¸¦ã³ï¼‰ --------------------
row2_author, row2_radio = st.columns([1.0, 2.0])   # â† è‘—è€…æ¬„ã‚’çŸ­ã‚ã«ã—ã¦ãƒ©ã‚¸ã‚ªã«å¹…ã‚’å¤šã‚ã«

with row2_radio:
    initials = ["ã™ã¹ã¦","ã‚","ã‹","ã•","ãŸ","ãª","ã¯","ã¾","ã‚„","ã‚‰","ã‚","è‹±å­—"]
    if "author_initial" not in st.session_state:
        st.session_state.author_initial = "ã™ã¹ã¦"
    st.radio(
    "è‘—è€…ã‚¤ãƒ‹ã‚·ãƒ£ãƒ«é¸æŠ",
    options=initials,
    horizontal=True,
    key="author_initial",   # â† ã“ã‚ŒãŒå”¯ä¸€ã®ã‚½ãƒ¼ã‚¹ã‚ªãƒ–ãƒˆã‚¥ãƒ«ãƒ¼ã‚¹
)

# ä»¥é™ã¯ session_state ã‹ã‚‰èª­ã‚€ã ã‘ï¼ˆä»£å…¥ã—ãªã„ï¼‰
ini = st.session_state["author_initial"]
# authors_readings.csv ã‚’èª­ã¿è¾¼ã¿
with row2_author:
    adf = load_authors_readings(AUTHORS_CSV_PATH)
    if adf is not None and not adf.empty:
        cand = adf.copy()

        # --- ï¼ˆä»¥ä¸‹ã¯å¾“æ¥ã¨åŒã˜ãƒ•ã‚£ãƒ«ã‚¿ï¼†ä¸¦ã³æ›¿ãˆå‡¦ç†ï¼‰---
        GOJUON = {
            "ã‚": "ã‚ã„ã†ãˆãŠ",
            "ã‹": "ã‹ããã‘ã“ãŒããã’ã”",
            "ã•": "ã•ã—ã™ã›ãã–ã˜ãšãœã",
            "ãŸ": "ãŸã¡ã¤ã¦ã¨ã ã¢ã¥ã§ã©",
            "ãª": "ãªã«ã¬ã­ã®",
            "ã¯": "ã¯ã²ãµã¸ã»ã°ã³ã¶ã¹ã¼ã±ã´ã·ãºã½",
            "ã¾": "ã¾ã¿ã‚€ã‚ã‚‚",
            "ã‚„": "ã‚„ã‚†ã‚ˆ",
            "ã‚‰": "ã‚‰ã‚Šã‚‹ã‚Œã‚",
            "ã‚": "ã‚ã‚’ã‚“",
        }

        def kata_to_hira(s: str) -> str:
            out = []
            for ch in str(s or ""):
                o = ord(ch)
                if 0x30A1 <= o <= 0x30F6:
                    out.append(chr(o - 0x60))
                else:
                    out.append(ch)
            return "".join(out)

        def hira_head(s: str) -> str | None:
            s = str(s or "")
            return kata_to_hira(s)[0] if s else None

        def is_roman_head(s: str) -> bool:
            return bool(re.match(r"[A-Za-z]", str(s or "")))

        ini = st.session_state.author_initial
        if ini == "è‹±å­—":
            cand = cand[cand["reading"].astype(str).str.match(r"[A-Za-z]")]
        elif ini != "ã™ã¹ã¦":
            allowed = set(GOJUON.get(ini, ""))
            cand = cand[cand["reading"].apply(
                lambda s: (not is_roman_head(s)) and (hira_head(s) in allowed if hira_head(s) else False)
            )]

        # ä¸¦ã³é †
        AIUEO_ORDER = "ã‚ã„ã†ãˆãŠã‹ããã‘ã“ã•ã—ã™ã›ããŸã¡ã¤ã¦ã¨ãªã«ã¬ã­ã®ã¯ã²ãµã¸ã»ã¾ã¿ã‚€ã‚ã‚‚ã‚„ã‚†ã‚ˆã‚‰ã‚Šã‚‹ã‚Œã‚ã‚ã‚’ã‚“"
        def sort_tuple(reading: str):
            if not reading: return (3, 999, "")
            ch = reading[0]
            if re.match(r"[A-Za-z]", ch): return (2, 999, ch.lower())
            if re.match(r"[\u30A0-\u30FF]", ch): return (1, 999, reading)
            idx = AIUEO_ORDER.find(ch)
            return (0, idx if idx != -1 else 998, reading)

        cand = cand.assign(
            _grp=[sort_tuple(r)[0] for r in cand["reading"]],
            _key=[sort_tuple(r)[1] for r in cand["reading"]],
            _sub=[sort_tuple(r)[2] for r in cand["reading"]],
        ).sort_values(by=["_grp","_key","_sub"], kind="mergesort").drop(columns=["_grp","_key","_sub"])

        reading2author = dict(zip(cand["reading"], cand["author"]))
        options_readings = list(reading2author.keys())

        authors_sel_readings = st.multiselect(
            "è‘—è€…ï¼ˆèª­ã¿ã§æ¤œç´¢å¯ / è¡¨ç¤ºã¯æ¼¢å­—ï¼‹èª­ã¿ï¼‰",
            options=options_readings,
            format_func=lambda r: f"{reading2author.get(r, r)}ï½œ{r}",
            placeholder="ä¾‹ï¼šã‚„ã¾ã  / ã•ã¨ã† / ãŸã‹ã¯ã— ..."
        )
        authors_sel = sorted({reading2author[r] for r in authors_sel_readings}) if authors_sel_readings else []
    else:
        authors_all = build_author_candidates(df)
        authors_sel = st.multiselect("è‘—è€…", authors_all, default=[])

# å¿µã®ãŸã‚æœªå®šç¾©ã‚¬ãƒ¼ãƒ‰
if 'authors_sel' not in locals(): authors_sel = []
if 'targets_sel' not in locals(): targets_sel = []
if 'types_sel'   not in locals(): types_sel   = []

# -------------------- æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆ3æ®µç›®ï¼šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼‰ --------------------
kw_row1, kw_row2 = st.columns([3, 1])
with kw_row1:
    kw_query = st.text_input("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆç©ºç™½/ã‚«ãƒ³ãƒã§è¤‡æ•°å¯ï¼‰", value="")
with kw_row2:
    kw_mode = st.radio("ä¸€è‡´æ¡ä»¶", ["OR", "AND"], index=0, horizontal=True, key="kw_mode")


# -------------------- ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨ --------------------
def apply_filters(_df: pd.DataFrame) -> pd.DataFrame:
    df2 = _df.copy()
    if "ç™ºè¡Œå¹´" in df2.columns:
        y = pd.to_numeric(df2["ç™ºè¡Œå¹´"], errors="coerce")
        df2 = df2[(y >= y_from) & (y <= y_to) | y.isna()]
    if vols_sel and "å·»æ•°" in df2.columns:
        df2 = df2[df2["å·»æ•°"].map(to_int_or_none).isin(set(vols_sel))]
    if issues_sel and "å·æ•°" in df2.columns:
        df2 = df2[df2["å·æ•°"].map(to_int_or_none).isin(set(issues_sel))]
    if authors_sel and "è‘—è€…" in df2.columns:
        sel = {norm_key(a) for a in authors_sel}
        def hit_author(v): return any(norm_key(x) in sel for x in split_authors(v))
        df2 = df2[df2["è‘—è€…"].apply(hit_author)]
    if targets_sel and "å¯¾è±¡ç‰©_top3" in df2.columns:
        t_norm = [norm_key(t) for t in targets_sel]
        df2 = df2[df2["å¯¾è±¡ç‰©_top3"].apply(lambda v: any(t in norm_key(v) for t in t_norm))]
    if types_sel and "ç ”ç©¶ã‚¿ã‚¤ãƒ—_top3" in df2.columns:
        t_norm = [norm_key(t) for t in types_sel]
        df2 = df2[df2["ç ”ç©¶ã‚¿ã‚¤ãƒ—_top3"].apply(lambda v: any(t in norm_key(v) for t in t_norm))]
    toks = tokens_from_query(kw_query)
    if toks:
        def hit_kw(row):
            hs = haystack(row)
            return all(t in hs for t in toks) if kw_mode == "AND" else any(t in hs for t in toks)
        df2 = df2[df2.apply(hit_kw, axis=1)]
    return df2

filtered = apply_filters(df)

# -------------------- æ¤œç´¢çµæœãƒ†ãƒ¼ãƒ–ãƒ« --------------------
st.markdown("### æ¤œç´¢çµæœ")
st.caption(f"{len(filtered)} / {len(df)} ä»¶")

visible_cols = make_visible_cols(filtered)

# â˜… ã“ã“ã§ summary ã®ä½ç½®ã‚’èª¿æ•´ï¼ˆè‘—è€…ã®å³ã«æŒ¿å…¥ï¼‰
if "è‘—è€…" in visible_cols and "summary" in filtered.columns:
    idx = visible_cols.index("è‘—è€…")
    if "summary" not in visible_cols:
        visible_cols.insert(idx + 1, "summary")

disp = filtered.loc[:, visible_cols].copy()
disp["_row_id"] = disp.apply(make_row_id, axis=1)

# ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆæœŸåŒ–ï¼šãŠæ°—ã«å…¥ã‚Šé›†åˆï¼ã‚¿ã‚°è¾æ›¸
if "favs" not in st.session_state:
    st.session_state.favs = set()
if "fav_tags" not in st.session_state:
    st.session_state.fav_tags = {}   # row_id -> set(tags)

# ãƒ¡ã‚¤ãƒ³è¡¨ï¼šãŠæ°—ã«å…¥ã‚Šãƒã‚§ãƒƒã‚¯åˆ—
disp["â˜…"] = disp["_row_id"].apply(lambda rid: rid in st.session_state.favs)

# LinkColumn è¨­å®š
column_config = {
    "â˜…": st.column_config.CheckboxColumn("â˜…", help="æ°—ã«ãªã‚‹è«–æ–‡ã«ãƒã‚§ãƒƒã‚¯/è§£é™¤", default=False, width="small"),
}
if "HPãƒªãƒ³ã‚¯å…ˆ" in disp.columns:
    column_config["HPãƒªãƒ³ã‚¯å…ˆ"] = st.column_config.LinkColumn("HPãƒªãƒ³ã‚¯å…ˆ", help="å¤–éƒ¨ã‚µã‚¤ãƒˆã¸ç§»å‹•", display_text="HP")
if "PDFãƒªãƒ³ã‚¯å…ˆ" in disp.columns:
    column_config["PDFãƒªãƒ³ã‚¯å…ˆ"] = st.column_config.LinkColumn("PDFãƒªãƒ³ã‚¯å…ˆ", help="PDFã‚’é–‹ã", display_text="PDF")

display_order = ["â˜…"] + [c for c in disp.columns if c not in ["â˜…", "_row_id"]] + ["_row_id"]

# --- ãƒ¡ã‚¤ãƒ³è¡¨ï¼ˆãƒ•ã‚©ãƒ¼ãƒ ã§ä¸€æ‹¬åæ˜ ï¼‰ ---
st.subheader("è«–æ–‡ãƒªã‚¹ãƒˆ")
with st.form("main_table_form", clear_on_submit=False):
    edited_main = st.data_editor(
        disp[display_order],
        key="main_editor",
        use_container_width=True,
        hide_index=True,
        column_config=column_config,
        disabled=[c for c in display_order if c != "â˜…"],  # â˜…ã®ã¿ç·¨é›†å¯
        height=520,
        num_rows="fixed",
    )
    apply_main = st.form_submit_button("ãƒã‚§ãƒƒã‚¯ã—ãŸè«–æ–‡ã‚’ãŠæ°—ã«å…¥ã‚Šãƒªã‚¹ãƒˆã«è¿½åŠ ", use_container_width=True)

if apply_main:
    subset_ids_main = set(disp["_row_id"].tolist())
    checked_subset_main = set(edited_main.loc[edited_main["â˜…"] == True, "_row_id"].tolist())
    st.session_state.favs = (st.session_state.favs - subset_ids_main) | checked_subset_main

# --- ãŠæ°—ã«å…¥ã‚Šä¸€è¦§ãƒ˜ãƒƒãƒ€ãƒ¼ï¼‹å…¨å¤–ã—ãƒœã‚¿ãƒ³ï¼ˆæ¨ªä¸¦ã³ï¼‰ ---
c1, c2 = st.columns([6, 1])
with c1:
    st.subheader(f"â­ ãŠæ°—ã«å…¥ã‚Šï¼ˆ{len(st.session_state.favs)} ä»¶ï¼‰")
with c2:
    if st.button("âŒ å…¨ã¦å¤–ã™", key="clear_favs_header", use_container_width=True):
        st.session_state.favs = set()
        st.rerun()

# ãŠæ°—ã«å…¥ã‚Šä¸€è¦§ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ç„¡è¦–ã§å…¨ä½“ã‹ã‚‰ï¼‰ï¼‹ tags åˆ—ï¼ˆç·¨é›†å¯ï¼‰
visible_cols_full = make_visible_cols(df)

# â˜… ã“ã¡ã‚‰ã‚‚åŒã˜ã‚ˆã†ã« summary ã‚’è‘—è€…ã®å³ã¸
if "è‘—è€…" in visible_cols_full and "summary" in df.columns:
    idx = visible_cols_full.index("è‘—è€…")
    if "summary" not in visible_cols_full:
        visible_cols_full.insert(idx + 1, "summary")

fav_disp_full = df.loc[:, visible_cols_full].copy()
fav_disp_full = df.loc[:, visible_cols_full].copy()
fav_disp_full["_row_id"] = fav_disp_full.apply(make_row_id, axis=1)
fav_disp = fav_disp_full[fav_disp_full["_row_id"].isin(st.session_state.favs)].copy()

def tags_str_for(rid: str) -> str:
    s = st.session_state.fav_tags.get(rid, set())
    return ", ".join(sorted(s)) if s else ""

if not fav_disp.empty:
    fav_disp["â˜…"] = fav_disp["_row_id"].apply(lambda rid: rid in st.session_state.favs)
    fav_disp["tags"] = fav_disp["_row_id"].apply(tags_str_for)  # â† è¡¨ç¤ºï¼†ç·¨é›†ã«ä½¿ã†

    fav_display_order = ["â˜…"] + [c for c in fav_disp.columns if c not in ["â˜…", "_row_id"]] + ["_row_id"]

    fav_column_config = {
        "â˜…": st.column_config.CheckboxColumn("â˜…", help="ãƒã‚§ãƒƒã‚¯ã§è§£é™¤/è¿½åŠ ï¼ˆä¸‹ã®ãƒœã‚¿ãƒ³ã§åæ˜ ï¼‰", default=True, width="small"),
        "tags": st.column_config.TextColumn("tagsï¼ˆã‚«ãƒ³ãƒ/ç©ºç™½åŒºåˆ‡ã‚Šï¼‰", help="ä¾‹: æ¸…é…’, ä¹³é…¸èŒ"),
    }
    if "HPãƒªãƒ³ã‚¯å…ˆ" in fav_disp.columns:
        fav_column_config["HPãƒªãƒ³ã‚¯å…ˆ"] = st.column_config.LinkColumn("HPãƒªãƒ³ã‚¯å…ˆ", display_text="HP")
    if "PDFãƒªãƒ³ã‚¯å…ˆ" in fav_disp.columns:
        fav_column_config["PDFãƒªãƒ³ã‚¯å…ˆ"] = st.column_config.LinkColumn("PDFãƒªãƒ³ã‚¯å…ˆ", display_text="PDF")

    # ãŠæ°—ã«å…¥ã‚Šè¡¨ï¼šâ˜…ã¨ tags ã®ã¿ç·¨é›†å¯
    with st.form("fav_table_form", clear_on_submit=False):
        fav_edited = st.data_editor(
            fav_disp[fav_display_order],
            key="fav_editor",
            use_container_width=True,
            hide_index=True,
            column_config=fav_column_config,
            disabled=[c for c in fav_display_order if c not in ["â˜…", "tags"]],  # â† tags ã‚’ç·¨é›†å¯ã«
            height=420,
            num_rows="fixed",
        )
        apply_fav = st.form_submit_button("ãŠæ°—ã«å…¥ã‚Šã®å¤‰æ›´ï¼ˆâ˜…/tagsï¼‰ã‚’æ›´æ–°", use_container_width=True)

    if apply_fav:
        # â˜…ã®æ›´æ–°
        subset_ids_fav = set(fav_disp["_row_id"].tolist())
        fav_checked_subset = set(fav_edited.loc[fav_edited["â˜…"] == True, "_row_id"].tolist())
        st.session_state.favs = (st.session_state.favs - subset_ids_fav) | fav_checked_subset

        # tags ã®æ›´æ–°ï¼ˆè¡Œã”ã¨ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ‘ãƒ¼ã‚¹ â†’ set ã«æ ¼ç´ï¼‰
        def parse_tags(s):
            if not isinstance(s, str): s = str(s or "")
            parts = [t.strip() for t in re.split(r"[ ,ï¼Œã€ï¼›;ã€€]+", s) if t.strip()]
            return set(parts)
        for _, r in fav_edited.iterrows():
            rid = r["_row_id"]
            tag_set = parse_tags(r.get("tags", ""))
            if tag_set:
                st.session_state.fav_tags[rid] = tag_set
            elif rid in st.session_state.fav_tags:
                # ç©ºã«ã—ãŸå ´åˆã¯å‰Šé™¤
                del st.session_state.fav_tags[rid]

        st.success("ãŠæ°—ã«å…¥ã‚Šï¼ˆâ˜…/tagsï¼‰ã‚’åæ˜ ã—ã¾ã—ãŸ")
        st.rerun()
else:
    st.info("ãŠæ°—ã«å…¥ã‚Šã¯æœªé¸æŠã§ã™ã€‚ä¸Šã®è¡¨ã®ã€â˜…ã€ã«ãƒã‚§ãƒƒã‚¯ã—ã¦ã‹ã‚‰åæ˜ ã—ã¦ãã ã•ã„ã€‚")
    fav_edited = None

# -------------------- ã‚¿ã‚°ã§ãŠæ°—ã«å…¥ã‚Šã‚’çµã‚Šè¾¼ã¿ï¼ˆAND/ORï¼‰ --------------------
with st.expander("ğŸ” ã‚¿ã‚°ã§ãŠæ°—ã«å…¥ã‚Šã‚’çµã‚Šè¾¼ã¿ï¼ˆAND/ORï¼‰", expanded=False):
    tag_query = st.text_input("ã‚¿ã‚°æ¤œç´¢ï¼ˆã‚«ãƒ³ãƒ/ç©ºç™½åŒºåˆ‡ã‚Šï¼‰", key="tag_query")
    tag_mode = st.radio("ä¸€è‡´æ¡ä»¶", ["OR", "AND"], index=0, horizontal=True, key="tag_mode")

    fav_disp_for_filter = fav_disp_full[fav_disp_full["_row_id"].isin(st.session_state.favs)].copy()
    if tag_query.strip():
        tags = [t.strip() for t in re.split(r"[ ,ï¼Œã€ï¼›;ã€€]+", tag_query) if t.strip()]
        def match_tags_row(row):
            row_tags = st.session_state.fav_tags.get(row["_row_id"], set())
            return all(t in row_tags for t in tags) if tag_mode == "AND" else any(t in row_tags for t in tags)
        fav_disp_for_filter = fav_disp_for_filter[fav_disp_for_filter.apply(match_tags_row, axis=1)]

    # è¡¨ç¤º
    def tags_str_for_filter(rid: str) -> str:
        s = st.session_state.fav_tags.get(rid, set())
        return ", ".join(sorted(s)) if s else ""
    fav_disp_for_filter["tags"] = fav_disp_for_filter["_row_id"].apply(tags_str_for_filter)

    show_cols = ["No.","ç™ºè¡Œå¹´","å·»æ•°","å·æ•°","è«–æ–‡ã‚¿ã‚¤ãƒˆãƒ«","è‘—è€…","å¯¾è±¡ç‰©_top3","ç ”ç©¶ã‚¿ã‚¤ãƒ—","HPãƒªãƒ³ã‚¯å…ˆ","PDFãƒªãƒ³ã‚¯å…ˆ","tags"]
    show_cols = [c for c in show_cols if c in fav_disp_for_filter.columns]
    st.dataframe(fav_disp_for_filter[show_cols], use_container_width=True, hide_index=True)

# -------------------- ä¸‹éƒ¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆCSVå‡ºåŠ›ï¼š2ç¨®é¡ï¼‰ --------------------
st.caption(
    f"ç¾åœ¨ã®ãŠæ°—ã«å…¥ã‚Šï¼š{len(st.session_state.favs)} ä»¶ / "
    f"ã‚¿ã‚°æ•°ï¼š{len({t for s in st.session_state.fav_tags.values() for t in s})} ç¨®"
)

# 1) çµã‚Šè¾¼ã¿çµæœã®å‡ºåŠ›ï¼ˆç”»é¢ã®æ¤œç´¢çµæœãƒ†ãƒ¼ãƒ–ãƒ«ã¨åŒã˜åˆ—ï¼‰
filtered_export_df = disp.drop(columns=["â˜…", "_row_id"], errors="ignore")

# 2) ãŠæ°—ã«å…¥ã‚Šã®å‡ºåŠ›ï¼ˆtags åˆ—ã‚’ä»˜ä¸ï¼‰
fav_export = fav_disp_full[fav_disp_full["_row_id"].isin(st.session_state.favs)].copy()

def _tags_join(rid: str) -> str:
    s = st.session_state.fav_tags.get(rid, set())
    return ", ".join(sorted(s)) if s else ""

fav_export["tags"] = fav_export["_row_id"].map(_tags_join)
fav_export = fav_export.drop(columns=["_row_id"], errors="ignore")

c_dl1, c_dl2 = st.columns(2)

with c_dl1:
    st.download_button(
        "ğŸ“¥ çµã‚Šè¾¼ã¿çµæœã‚’CSVå‡ºåŠ›ï¼ˆè¡¨ç¤ºåˆ—ã®ã¿ï¼‰",
        data=filtered_export_df.to_csv(index=False).encode("utf-8-sig"),
        file_name=f"filtered_{time.strftime('%Y%m%d')}.csv",
        mime="text/csv",
        use_container_width=True
    )

with c_dl2:
    st.download_button(
        "â­ ãŠæ°—ã«å…¥ã‚Šã‚’CSVå‡ºåŠ›ï¼ˆtagsä»˜ãï¼‰",
        data=fav_export.to_csv(index=False).encode("utf-8-sig"),
        file_name=f"favorites_{time.strftime('%Y%m%d')}.csv",
        mime="text/csv",
        use_container_width=True,
        disabled=fav_export.empty
    )